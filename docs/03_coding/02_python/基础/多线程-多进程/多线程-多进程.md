# å¤šçº¿ç¨‹/å¤šè¿›ç¨‹

## ç›®å½•

-   [multiprocessä½¿ç”¨å¿ƒå¾—ï¼š](#multiprocessä½¿ç”¨å¿ƒå¾—)

> ğŸ“Œpython å› ä¸ºæœ‰çº¿ç¨‹é”ï¼Œæ‰€ä»¥æ²¡æœ‰å¤šçº¿ç¨‹ï¼Œåªæœ‰å¤šè¿›ç¨‹

[pythonå¹¶è¡Œè®¡ç®—ï¼ˆå®Œç»“ç¯‡ï¼‰ï¼šå¹¶è¡Œæ–¹æ³•æ€»ç»“ - çŸ¥ä¹](pythonå¹¶è¡Œè®¡ç®—ï¼ˆå®Œç»“ç¯‡ï¼‰ï¼šå¹¶è¡Œæ–¹æ³•æ€»ç»“%20-%20çŸ¥ä¹.md - çŸ¥ä¹/pythonå¹¶è¡Œè®¡ç®—ï¼ˆå®Œç»“ç¯‡ï¼‰ï¼šå¹¶è¡Œæ–¹æ³•æ€»ç»“ - çŸ¥ä¹.md> "pythonå¹¶è¡Œè®¡ç®—ï¼ˆå®Œç»“ç¯‡ï¼‰ï¼šå¹¶è¡Œæ–¹æ³•æ€»ç»“ - çŸ¥ä¹")

[pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸Šï¼‰ï¼šmultiprocessingã€multiprocessæ¨¡å— - çŸ¥ä¹](pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸Šï¼‰ï¼šmultiprocessingã€multiprocessæ¨¡å—%20-%20çŸ¥ä¹.md - çŸ¥ä¹.md> "pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸Šï¼‰ï¼šmultiprocessingã€multiprocessæ¨¡å— - çŸ¥ä¹")

[pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸‹ï¼‰ï¼šmultiprocessingæ¨¡å—å®ä¾‹ - çŸ¥ä¹](pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸‹ï¼‰ï¼šmultiprocessingæ¨¡å—å®ä¾‹%20-%20çŸ¥ä¹.md - çŸ¥ä¹.md> "pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸‹ï¼‰ï¼šmultiprocessingæ¨¡å—å®ä¾‹ - çŸ¥ä¹")

[pandas groupby apply å¹¶è¡Œå¤„ç†](pandas%20groupby%20apply%20å¹¶è¡Œå¤„ç†.md groupby apply å¹¶è¡Œå¤„ç†/pandas groupby apply å¹¶è¡Œå¤„ç†.md> "pandas groupby apply å¹¶è¡Œå¤„ç†")

[Pandas groupbyåŠ é€Ÿå¤„ç†æ•°æ®é—®é¢˜\_pd.groupby è¿è¡Œé€Ÿåº¦-CSDNåšå®¢](https://blog.csdn.net/Pual_wang/article/details/106523653 "Pandas groupbyåŠ é€Ÿå¤„ç†æ•°æ®é—®é¢˜_pd.groupby è¿è¡Œé€Ÿåº¦-CSDNåšå®¢")

## multiprocessä½¿ç”¨å¿ƒå¾—ï¼š

æœ€å¥½å…ˆæŠŠ pd.Dataframe group byä¹‹åçš„æ•°æ®åˆ†ç»„ï¼Œä¸”æŒ‰ç…§è®¡ç®—æœºprocessçš„ä¸ªæ•°åˆ†å¥½ç»„ï¼Œå†è°ƒç”¨multiprocessã€‚å¦åˆ™ï¼Œå½“ä»»åŠ¡å¤ªå¤šä¹‹åï¼Œmultiprocessä¸ä¸€å®šèƒ½åˆ†é…æˆåŠŸã€‚

> ä»¥ä¸‹code byé»„å°çŒ«ï¼Œå‚è€ƒï¼š[pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸‹ï¼‰ï¼šmultiprocessingæ¨¡å—å®ä¾‹ - çŸ¥ä¹](pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸‹ï¼‰ï¼šmultiprocessingæ¨¡å—å®ä¾‹%20-%20çŸ¥ä¹.md - çŸ¥ä¹.md> "pythonå¹¶è¡Œè®¡ç®—ï¼ˆä¸‹ï¼‰ï¼šmultiprocessingæ¨¡å—å®ä¾‹ - çŸ¥ä¹")

```python
## ä»¥ä¸‹çš„ä»£ç æ—¢å¯ä»¥å•çº¿ç¨‹ä¹Ÿå¯ä»¥
def fun(args):
    if is_multi_process:
        (flts, process_idx, conn, t0) = args
    else:
        (flts) = args
    res = []
    for (keys, flt) in flts:
        ## æŒ‰group byè¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°_resultï¼Œå‡è®¾ä¸º_tmp_dict,X_train_df,y_train
        res.append((_tmp_dict,X_train_df,y_train))
        
    if is_multi_process:
        t = time.time() - t0
        conn.send('data process of %d: finished@%.2fs' % (process_idx+1,t))
    return res

def multi_process():
    res = []
    p = multiprocessing.Pool()
    p_conn, c_conn = multiprocessing.Pipe()
    params = []
    t0 = time.time()

    # total_flts = len(train_data.groupby(by=train_group_col).size())
    _njobs = multiprocessing.cpu_count()
    ## ---- æ ¹æ®cpuçš„è¿›ç¨‹æ•°åˆ†ç»„
    _flight_group_list = []
    for i in range(_njobs):
        _flight_group_list.append([])
    i =0
    for keys,flt in train_data.groupby(by=train_group_col):
        _flight_group_list[i%_njobs].append((keys,flt))
        i += 1

    for i in range(_njobs):
        args = (_flight_group_list[i],i, c_conn,t0)
        params.append(args)

    ## ---- è¿è¡Œå¤šçº¿ç¨‹
    res = p.map(fun, params)
    p.close()
    p.join()

    print('output:')
    while p_conn.poll():
        print(p_conn.recv())
    
    ## ---- æ±‡æ€»è¿è¡Œå®Œçš„æ•°æ®
    param_dict = {}
    for _res in res:  # å¯¹äºæ¯ä¸€ä¸ªè¿›ç¨‹å¾—åˆ°çš„ç»“æœ
        for (_tmp_dict,X_train_df,y_train_df) in _res:
            param_dict[_tmp_dict['keys']] = {'intercept': _tmp_dict['intercept'], 'coef':_tmp_dict['coef']}
    print(param_dict)

```
