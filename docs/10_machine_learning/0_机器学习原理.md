[TOC]

## 0. Intro

"没有免费的午餐"定理（No Free Lunch Theorem）：没有哪个模型在所有问题中都表现得更好。



对于监督学习，如果y是numerical variable，方法称之为 regression。

forecast仅针对时间序列数据，其他都叫prediction



如何选择模型，可以参考sklearn的图：

![sklearn关于ML模型选择的建议](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/sklearn%E5%85%B3%E4%BA%8EML%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E7%9A%84%E5%BB%BA%E8%AE%AE.png)

## 1. 性能度量

### 1.1 测试集与训练集

西瓜书p25

#### 留出法

- 如有必要，分层采样
- 训练集 2/3 ~ 4/5

#### 交叉验证

- k-fold cross validation K-折

  常见的：10次10折

- leave-one-out 留一

#### 自助法--数据集较小时使用



#### 超参调节

调参：parameter tuning

算法的参数称为“超参数”

参考：[`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection).[validation_curve](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) 

![使用 SVM 的验证曲线](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/sphx_glr_plot_validation_curve_001.png)

橙色为训练集的score，可以看到当参数gamma越高，score越高。在gamma较小时，为欠拟合。

通过交叉验证（蓝色）score，可以看到泛化误差，在gamma较大时，过拟合，泛化误差较大。



#### 学习曲线

当训练集数量变化时，训练和验证的分数。用来衡量从更多训练数据中获益多少，以及是否更容易受到方差或偏差的影响。

![img](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/0wIaoQ-vXhW-Oxbf9.png)

ref：https://hshan0103.medium.com/understanding-bias-variance-trade-off-from-learning-curve-a64b4223bb02

- 当只有一个数据点，拟合=数据点，训练误差为0，但验证错误高；

- 训练集增加，无法完美拟合，训练误差增加；但捕获有用信息，验证误差减小；
- 最佳训练数据大小是曲线在其之后变得平稳的点。进一步增加规模只会降低训练过程的效率，没有任何改善。






### 1.2 性能度量

参考：sklearn https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation

#### 1--分类问题

|                    | **预测结果**--正例 | **预测结果**--反例 |
| :----------------: | :----------------: | :----------------: |
| **真实情况**--正例 |    TP( 真正例 )    |    FN( 假反例 )    |
| **真实情况**--反例 |    FP (假正例 )    |    TN (真反例 )    |

##### 精确度

错误率，精度

查准率P（预测为真的那些，判断对的概率）、查全率R（实际为真的那些，判断对的概率）

$P=\frac{TP}{TP+FP},R=\frac{TP}{TP+FN}$

![image-20210928152827864](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20210928152827864.png)

$F1=\frac{2×P×R}{P+R}$

##### ROC & AUC

ROC--Receiver Operating Characteristics

![image-20210928152843793](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20210928152843793.png)

AUC--area under ROC Curve，为ROC曲线下的面积，==越大越好==







#### 2--连续y



### 1.3 欠拟合与过拟合

![04a3TtAF0gL1XYXa6](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/04a3TtAF0gL1XYXa6.jpeg)

sklearn的多项式回归拟合的例子非常能够说明这一问题：

[Underfitting vs. Overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)

三角函数用4次多项式拟合刚好，1次多项式欠拟合，15次多项式过拟合：



![overfit_underfit](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/overfit_underfit.png)





### 1.4 偏差与方差

> bias-variance decomposition

真实值 $y$，预测值 $f(\boldsymbol{x} ; D)$

使用样本数相同的不同训练集产生的方差为
$$
\operatorname{var}(\boldsymbol{x})=\mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]
$$
噪声为
$$
\varepsilon^{2}=\mathbb{E}_{D}\left[\left(y_{D}-y\right)^{2}\right]
$$
期望输出与真实标记的差别称为偏差(bias), 即
$$
\operatorname{bias}^{2}(\boldsymbol{x})=(\bar{f}(\boldsymbol{x})-y)^{2}
$$
假定噪声期望为零, 即 $\mathbb{E}_{D}\left[y_{D}-y\right]=0 .$ 则
$$
E(f ; D)=\operatorname{bias}^{2}(\boldsymbol{x})+\operatorname{var}(\boldsymbol{x})+\varepsilon^{2}
$$
泛化误差可分解为偏差、方差与噪声之和。



#### 偏差vs.方差   and   学习曲线

参考：[scikit-learn----validation curves](https://scikit-learn.org/stable/modules/learning_curve.html)

- 欠拟合：偏差大
- 过拟合：方差大，对变化的训练数据（高方差）非常敏感。

![0DBnHNG62R2mMjZ4Z](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/0DBnHNG62R2mMjZ4Z.png)

![0C_A91jYMRfOsBZaU](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/0C_A91jYMRfOsBZaU.png)

ref: https://hshan0103.medium.com/understanding-bias-variance-trade-off-from-learning-curve-a64b4223bb02

- 训练误差-->偏差

- 训练误差与验证误差之间的差距-->方差



**高偏差，低方差——欠拟合：**

- 增加训练实例大多没用。该模型甚至不能很好地拟合当前的数据集。
- 通过减少正则化或增加特征输入的维度来**增加模型复杂性**。

**低偏差，高方差——过拟合：**

- 增加训练实例有用——模型很好地拟合了当前的训练数据集，增加数据集可能会捕获额外信息从而提高泛化性。
- 通过增加正则化或特征选择来**降低模型复杂度**（减少特征框架维度）



#### 学习曲线



sklearn.model_selection.learning_curve(*estimator*, *X*, *y*, ***, *groups=None*, *train_sizes=array([0.1, 0.33, 0.55, 0.78, 1.])*, *cv=None*, *scoring=None*, *exploit_incremental_learning=False*, *n_jobs=None*, *pre_dispatch='all'*, *verbose=0*, *shuffle=False*, *random_state=None*, *error_score=nan*, *return_times=False*, *fit_params=None*)[[source\]](https://github.com/scikit-learn/scikit-learn/blob/844b4be24/sklearn/model_selection/_validation.py#L1346)[¶](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve)

```python
train_sizes, train_scores, test_scores, fit_times, _ = \
        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_sizes,
                       return_times=True)
   ## 注：对于regression而言，默认的scoring是r2
```



[sklearn绘制学习曲线](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)



**图1：**这个学习曲线的score = 1-Error，所以上面那条红色的是训练集的准确率；下面那根绿色是测试集的准确率；

如果红色拟合的很好（低偏差），但红绿之间的间隔比较大（高方差）就表示过拟合了

**图2：**显示了测试集数量上升之后的拟合时间。

**图3：** 显示了测试集的准确率与拟合时间之间的关系。

![sphx_glr_plot_learning_curve_001](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/sphx_glr_plot_learning_curve_001.png)





例如，决策树容易过拟合（下图中的低偏差，高方差）：

![image-20211022155721602](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211022155721602.png)

相比之下，XGboost就好很多

![image-20211022155820058](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211022155820058.png)

而且可以看出来，8000-10000的训练集已经足够好了。







## 2. 单学习器



## 3. 集成学习 ensemble learning

结合多个学习器。

- 同质homogeneous集成：仅包含同种类型的个体学习器，如“决策树集成”“神经网络集成”。个体称为“基学习器”base learner
- 异质heterogeneous集成：包含多种类型，个体称为“组件学习器”component learner

可分为两类：

1. 个体之间存在强依赖关系，必须串行进行，代表：**Boosting**；
2. 个体之间无强依赖关系，可并行进行，代表：**Bagging** & **随机森林**；



### Bagging

![38f3d18e-81a9-471f-9a1c-3172b5fa3246_5](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/38f3d18e-81a9-471f-9a1c-3172b5fa3246_5.jpg)

### Boosting



![a9a5ff4e-b617-4afe-b27b-d96793defa87_6](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/a9a5ff4e-b617-4afe-b27b-d96793defa87_6.jpg)

著名代表：AdaBoost

　boosting的算法原理我们可以用一张图做一个概括如下：从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。　　　　

Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。

![56a97436-3f9d-47b0-9a09-2dfadef5253d_8](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/56a97436-3f9d-47b0-9a09-2dfadef5253d_8.jpg)



