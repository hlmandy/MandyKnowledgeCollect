[TOC]

##### 参考资料

**1. 基础SVM**

先看 2021_An Introduction To Statistical Learning with R_2nd_2021_stanford教材.pdf

再看 西瓜书

也可以参考wiki: [Support vector machine - Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)

李航的《统计学方法》写得让人不太看得懂

> 有一本经典的书在豆丁：[Statistical Learning Theory - Vapnik - 1998 - 豆丁网 (docin.com)](https://www.docin.com/p-31847626.html)，但是看上面的就够了

**2. imbalanced**

Fernandez_2018_Book_LearningFromImbalancedDataSets.pdf 中 4.5.1 节

sklearn中的SVC包含了weighted SVM 功能，主要来自于LIBSVM这个包，如下：

Chang and Lin, [LIBSVM: A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). Chang, C. C., & Lin, C. J. (2011). LIBSVM: a library for support vector machines. *ACM transactions on intelligent systems and technology (TIST)*, *2*(3), 1-27.

> [1.4. Support Vector Machines — scikit-learn 1.3.0 documentation](https://scikit-learn.org/stable/modules/svm.html)

> [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (but not [`NuSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC)) implements the parameter `class_weight` in the `fit` method. It’s a dictionary of the form `{class_label : value}`, where value is a floating point number > 0 that sets the parameter `C` of class `class_label` to `C * value`. The figure below illustrates the decision boundary of an unbalanced problem, with and without weight correction.
>
> sklearn.svm.SVC [sklearn.svm.SVC — scikit-learn 1.3.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)
>
> > C-Support Vector Classification. The implementation is based on libsvm.



## 基本原理

对于 $p$ 维的X,Y，找到一个 $p-1$ 维的超平面，$wx+b=0$，超平面的一边为$wx+b>0$，另一边为$wx+b<0$

预测值为，如果 $wx+b>0$，则 $y=1$；如果 $wx+b<0$，则 $y = -1$。

目标：x距离这个超平面越远越好。这个距离就叫margin。

![image-20230702133954338](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230702133954338.png)

### 求解

![image-20230702134211729](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230702134211729.png)

> 公式ref：2021_An Introduction To Statistical Learning with R_2nd_2021_stanford教材.pdf

用拉格朗日等方法来求解。

### 核函数

图中的超平面是affine function，是线性的。如果用核函数，就是非线性的了。

### soft margin

如果找不到这样的超平面，可以放松约束，并对违背margin的进行惩罚（违背margin指的是：所有的点都应该在margin外）。

这叫做soft margin。

![image-20230702134249392](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230702134249392.png)

![image-20230702134300853](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230702134300853.png)

> 公式ref：2021_An Introduction To Statistical Learning with R_2nd_2021_stanford教材.pdf



## LIBSVM

2.1 C-Support Vector Classification
Given training vectors $\boldsymbol{x}_i \in R^n, i=1, \ldots, l$, in two classes, and an indicator vector $\boldsymbol{y} \in R^l$ such that $y_i \in\{1,-1\}, C$-SVC (Boser et al., 1992; Cortes and Vapnik, 1995) solves the following primal optimization problem.
$$
\begin{aligned}
\min _{\boldsymbol{w}, b, \boldsymbol{\xi}} & \frac{1}{2} \boldsymbol{w}^T \boldsymbol{w}+C \sum_{i=1}^l \xi_i \\
\text { subject to } & y_i\left(\boldsymbol{w}^T \phi\left(\boldsymbol{x}_i\right)+b\right) \geq 1-\xi_i \\
& \xi_i \geq 0, i=1, \ldots, l,
\end{aligned}
$$

所以在SVC里面，C是cost



6 Unbalanced Data and Solving the Two-variable Sub-problem
$$
\begin{aligned}
\min _{\boldsymbol{w}, b, \boldsymbol{\xi}} & \frac{1}{2} \boldsymbol{w}^T \boldsymbol{w}+C^{+} \sum_{y_i=1} \xi_i+C^{-} \sum_{y_i=-1} \xi_i \\
\text { subject to } \quad & y_i\left(\boldsymbol{w}^T \phi\left(\boldsymbol{x}_i\right)+b\right) \geq 1-\xi_i, \\
& \xi_i \geq 0, i=1, \ldots, l,
\end{aligned}
$$
where $C^{+}$and $C^{-}$are regularization parameters for positive and negative classes, respectively. LIBSVM supports this setting, so users can choose weights for classes.

9 parameter selection 超参

To train SVM problems, users must specify some parameters. LIBSVM provides a simple tool to check a grid of parameters. For each parameter setting, LIBSVM obtains cross-validation (CV) accuracy. Finally, the parameters with the highest CV accuracy are returned. The parameter selection tool assumes that the RBF (Gaussian) kernel is used although extensions to other kernels and SVR can be easily made. The RBF kernel takes the form
$$
K\left(\boldsymbol{x}_i, \boldsymbol{x}_j\right)=e^{-\gamma\left\|x_i-\boldsymbol{x}_j\right\|^2}
$$
so $(C, \gamma)$ are parameters to be decided. Users can provide a possible interval of $C$ (or $\gamma$ ) with the grid space. Then, all grid points of $(C, \gamma)$ are tried to find the one giving the highest $\mathrm{CV}$ accuracy. Users then use the best parameters to train the whole training set and generate the final model.

主要要决策 C 和 $\gamma$

ref: 西瓜书：

$k(x_i,x_j)=\phi(x_i)^T\phi(x_j)$

核函数表示两个向量之间的相似性








## Python

### sklearn

https://scikit-learn.org/stable/modules/svm.html

用SVC 里面有 `class_weight`

#### SVC参数解释

**1. kernel**

[sklearn.svm.SVC — scikit-learn 1.3.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)

默认的核函数为"rbf" Radial basis function kernel，也就是高斯核函数

> [Radial basis function kernel - Wikipedia](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)

**2. class_weight**



**超参**

```python
param_grid ``=` `{``'C'``: [``0.1``, ``1``, ``10``, ``100``, ``1000``], 
       ``'gamma'``: [``1``, ``0.1``, ``0.01``, ``0.001``, ``0.0001``],
       ``'kernel'``: [``'rbf'``]} 
```

C是cost

gamma是核函数的系数













