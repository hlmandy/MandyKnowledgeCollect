 ref: 

1. Introduction to Data Mining -- second edition (Pang-Ning Tan)
2. 机器学习西瓜书  & pumpkin
3. Introduction to Machine Learning with Python  



[TOC]

# 决策树

样本纯度 purity：一个集合中的元素尽可能属于同一类别。信息熵 information entropy 是衡量样本集合纯度最常用的一种指标。

## 信息熵

$$\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_{2}{p_k}$$

$p_k$ 是第 $k$ 类样本所占的比例。

> $0\leq\operatorname{Ent}(D)\leq\log_{2}|\mathcal{Y}|$
> 当个体在$|\mathcal{Y}|$ 均匀分布，则信息熵越大，为$log_{2}|\mathcal{Y}|$
>
> 当所有个体为同一组，则信息熵最小，为0



![image-20220623163906099](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220623163906099.png)

> Shannon 的信息熵模型就是 $-log_2P(x)$ 的一个加权回归



假设离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1,a^2,...,a^V\}$，对应 $V$ 个分支，每个分支 $a^v$ 的集合为 $D^v$。那么使用属性 $a$ 进行分支的信息增益：

$$\operatorname{Gain}(D,a) = \operatorname{Ent}(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\operatorname{Ent}({D^v})$$

$\operatorname{Gain}(D,a)$ 表示用 $a$ 对集合 $D$ 进行划分所获得的“纯度提升”（不确定性减少的程度）。



## 决策数算法一览

`ref: sklean` [1.10.6. 决策树算法: ID3, C4.5, C5.0 和 CART](https://www.sklearncn.cn/11/)

所有种类的决策树算法有哪些以及它们之间的区别？scikit-learn 中实现何种算法呢？

**[ID3](https://en.wikipedia.org/wiki/ID3_algorithm)（Iterative Dichotomiser 3）**由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。

**C4.5 是 ID3 的后继者**，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。

**C5.0** 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。

**[CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)（Classification and Regression Trees （分类和回归树）**）与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。

scikit-learn 使用 CART 算法的优化版本。



### ID3决策树算法：(Quinlan,1986)

 **选择信息增益最大的属性进行分支。**

**该算法的弊端：**对于可取值数目较多的属性有偏好。

```
# 该方法是基于递归、深度搜索树

```



注：level 指的是离散变量的不同取值，target level就是目标 y 的不同取值

![image-20220623170445457](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220623170445457.png)

![image-20220623170755276](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220623170755276.png)



### C4.5算法(Quinlan,1993)

**不直接使用信息增益，而是使用“增益率” gain ratio：**

$$\operatorname{Gain\_ratio}(D,a) = \frac{\operatorname{Gain}(D,a)}{\operatorname{IV}(a)}$$

$$\operatorname{IV}(a) = -\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_{2}{\frac{|D^v|}{|D|}}$$，在(Quinlan,1993)中称之为 split info(a) `page 23`

IV：固有值，intristic value。属性 $a$ 可能的取值数目越多（V越大），则 IV 越大。

> $$-\sum_{v=1}^{V}p\log_{2}{p}$$
>
> 这个函数中， $p$ 越小，函数值越大。
>
> 当 $a$ 可能的取值数目越多, $\frac{|D^v|}{|D|}$越小，IV越大。IV大了之后，Gain_ratio就变小。

**该算法的弊端：**对于可取值数目**较少**的属性有偏好。因此，要先选 Gain 大于平均水平的，再从中选择 Gain_ratio 大的。



### CART决策树(Breiman 1984) —— Gini Index

> CART: classification and regression tree
>
> 既可以用来分类，也可以用来回归

CART 假设决策树是二叉树，左边为是，右边为否。

**步骤：**

1. 生成决策树
2. 决策树剪枝

#### 基尼系数：反映了从一个集合里面，随机抽取两个样本，类别不一致的概率

$$\begin{aligned}\operatorname{Gini}(D)&=\sum_{k=1}^{|y|}\sum_{{k'}\neq k}{p_k}{p_{k'}}\\
&=1-\sum_{k=1}^{|y|}p_k^2
\end{aligned}$$

那么属性 $a$ 的基尼系数为：

$$\operatorname{Gini\_index}(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}\operatorname{Gini}(D^v)$$

> 注意！实际上CART用的是misclassification cost。只有当cost相同的时候，才是gini

#### Class assignment rule (from model to rule) `CART1984-sec 2.4.3`每个node是哪个class？

1. 不考虑数据imbalance的默认做法是：每个leaf node $t$ 的类别由数量最多的决定：$max_j p(j|t)$。如果 $p(j_0|t)=max_j p(j|t)$，则这个leaf node的类别为 $j_0$。

2. 如果考虑imbalance，则以misclassification cost最小的类别标记该叶子节点。

   具体操作如下：

   如果标记 $c(i|j)$ 为当把class j 分成 class i 的成本，且 

- $c(i|j)\ge 0, i \neq j$
- $c(i|j)=0, i=j$

  则如果把这个leaf node分为 i， misclassification的总成本为

  $\sum_j c(i|j)p(j|t)$

  所以就要找到 $i_0$，使得上式最小。

  正式定义：

  $r(t) = min_i\sum_j c(i|j)p(j|t)$

  该树的总 misclassification cost is:

  $R(T) = \sum_{t\in\tilde{T}}r(t)p(t) = \sum_{t\in\tilde{T}}R(t)$

  其中，$\tilde{T}$ 表示leaf nodes 集合。 

## 剪枝处理 pruning

- **pre**pruning 预剪枝: 在生成的过程中，估计划分，然后标记为叶节点；

  > 预剪枝基于贪心本质展开，有欠拟合的风险

- **post**pruning: 生成之后，再把子树替换为叶节点

  > 后剪枝是用验证集来剪枝的

通过泛化性能来判断是否需要剪枝。要把训练集分为 **训练集training set** 和 **验证集 validation set**



### CART 剪枝

> ref: 统计学习方法 —— 李航

> Highlight:
>
> - **当 $\alpha$ 大 的时候, 最优子树 $T_{\alpha}$ 偏小; 当 $\alpha$ 小的时候, 最优子树 $T_{\alpha}$ 偏大**
> - **剪枝后得到 $\alpha$ 逐渐增大的嵌套子树；$\alpha$ 和子树一一对应**

CART 剪枝算法从 “完全生长” 的决策树的底端剪去一些子树, 使决策树变小 (模 型变简单), 从而能够对末知数据有更准确的预测。CART 剪枝算法由两步组成: 首先 从生成算法产生的决策树 $T_{0}$ 底端开始不断剪枝, 直到 $T_{0}$ 的根结点, 形成一个子树序 列 $\left\{T_{0}, T_{1}, \cdots, T_{n}\right\}$; 然后通过交叉验证法在独立的验证数据集上对子树序列进行测 试, 从中选择最优子树。

#### 1. 剪枝, 形成一个子树序列

在剪枝过程中, 计算**子树的损失函数:**
$$
C_{\alpha}(T)=C(T)+\alpha|T|
$$
其中, $T$ 为任意子树, $C(T)$ 为对训练数据的预测误差 (如基尼指数), $|T|$ 为子树的叶 结点个数, $\alpha \geqslant 0$ 为参数, $C_{\alpha}(T)$ 为参数是 $\alpha$ 时的子树 $T$ 的整体损失。参数 $\alpha$ 权衡训 练数据的拟合程度与模型的复杂度。
对固定的 $\alpha$, 一定存在使损失函数 $C_{\alpha}(T)$ 最小的子树, 将其表示为 $T_{\alpha} \circ T_{\alpha}$ 在损 失函数 $C_{\alpha}(T)$ 最小的意义下是最优的。容易验证这样的最优子树是唯一的。**当 $\alpha$ 大 的时候, 最优子树 $T_{\alpha}$ 偏小; 当 $\alpha$ 小的时候, 最优子树 $T_{\alpha}$ 偏大**。极端情况, 当 $\alpha=0$ 时, 整体树是最优的。当 $\alpha \rightarrow \infty$ 时, 根结点组成的单结点树是最优的。

Breiman 等人证明: 可以用递归的方法对树进行剪枝。将 $\alpha$ 从小增大, $0=\alpha_{0}<$ $\alpha_{1}<\cdots<\alpha_{n}<+\infty$, 产生一系列的区间 $\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n$; 剪枝得到的子树序列对应着区间 $\alpha \in\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n$ 的最优子树序列 $\left\{T_{0}, T_{1}, \cdots, T_{n}\right\}$, 序 列中的子树是嵌套的。
具体地, 从整体树 $T_{0}$ 开始前枝。对 $T_{0}$ 的任意内部结点 $t$, 以 $t$ 为单结点（叶节点）树的损失 函数是
$$
C_{\alpha}(t)=C(t)+\alpha
$$
以 $t$ 为根结点的子树 $T_{t}$ 的损失函数是
$$
C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha\left|T_{t}\right|
$$
当 $\alpha=0$ 及 $\alpha$ 充分小时, 有不等式（5.29）
$$
C_{\alpha}\left(T_{t}\right)<C_{\alpha}(t)
$$
当 $\alpha$ 增大时, 在某一 $\alpha$ 有
$$
C_{\alpha}\left(T_{t}\right)=C_{\alpha}(t)
$$
当 $\alpha$ 再增大时, 不等式 (5.29) 反向。只要 $\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}, T_{t}$ 与 $t$ 有相同的 损失函数值, 而 $t$ 的结点少, 因此 $t$ 比 $T_{t}$ 更可取, 对 $T_{t}$ 进行剪枝。
为此, 对 $T_{0}$ 中每一内部结点 $t$, 计算
$$
g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
$$
**它表示剪枝后整体损失函数减少的程度。**在 $T_{0}$ 中前去 $g(t)$ 最小的 $T_{t}$, 将得到的子树 作为 $T_{1}$, 同时将最小的 $g(t)$ 设为 $\alpha_{1} 。 T_{1}$ 为区间 $\left[\alpha_{1}, \alpha_{2}\right)$ 的最优子树。
如此前枝下去, 直至得到根结点。在这一过程中, 不断地增加 $\alpha$ 的值, 产生新的 区间。

#### 2. 在剪枝得到的子树序列 $T_{0}, T_{1}, \cdots, T_{n}$ 中通过交叉验证选取最优子树 $T_{\alpha}$

具体地, 利用独立的验证数据集, 测试子树序列 $T_{0}, T_{1}, \cdots, T_{n}$ 中各棵子树的平 方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子 树序列中, 每棵子树 $T_{1}, T_{2}, \cdots, T_{n}$ 都对应于一个参数 $\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n}$ 。所以, **当最优 子树 $T_{k}$ 确定时, 对应的 $\alpha_{k}$ 也确定了, 即得到最优决策树 $T_{\alpha}$ 。**
现在写出 CART 剪枝算法。



> 算法 $5.7$ (CART 剪枝算法)
> 输入: CART 算法生成的决策树 $T_{0}$;
> 输出: 最优决策树 $T_{\alpha}$ 。
> （1）设 $k=0, T=T_{0}$ 。
> （2）设 $\alpha=+\infty$ 。
> （3）自下而上地对各内部结点 $t$ 计算 $C\left(T_{t}\right),\left|T_{t}\right|$ 以及
> $$
> \begin{aligned}
> g(t) &=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \\
> \alpha &=\min (\alpha, g(t))
> \end{aligned}
> $$
> 这里, $T_{t}$ 表示以 $t$ 为根结点的子树, $C\left(T_{t}\right)$ 是对训练数据的预测误差, $\left|T_{t}\right|$ 是 $T_{t}$ 的叶 结点个数。
> （4）对 $g(t)=\alpha$ 的内部结点 $t$ 进行前枝, 并对叶结点 $t$ 以多数表决法决定其类, 得到树 $T$ 。
> （5）设 $k=k+1, \alpha_{k}=\alpha, T_{k}=T$ 。
> （6）如果 $T_{k}$ 不是由根结点及两个叶结点构成的树, 则回到步骤 (2); 否则令 $T_{k}=T_{n}$ 。
> （7）采用交叉验证法在子树序列 $T_{0}, T_{1}, \cdots, T_{n}$ 中选取最优子树 $T_{\alpha}$ 。





## cost sensitive learning

关于cost-sensitive learning，相关的还有imbalance dataset的问题（就是不同class的instance数量差距比较大，例如得癌症的人很少之类，而往往想把这类人都找到，把得癌症的分为不得癌症的人的cost会非常高）

所以cost-sensitive和imbalance dataset这两个问题经常同时出现。



### cost-sensitive CART

其实早在CART树里面，就考虑了非对称的misclassification cost这个问题。

<u>CART认为在生成树的过程中，使用Gini更好；而在prune的时候，则考虑misclassification cost</u>

(第四章有提到，在生成树的时候，splitting用什么方法没多大差别。)

**注意，CART里面还提到了一个prior参数，没看懂。既然说生成树区别不大，暂且跳过**







## 连续值处理

### unsupervised

例如等距离（equal width）、等分布（equal frequency/depth）

### supervised

> Quinlan, 1993

假设连续值 $a$ 出现了 n 种可能性，从小到大排序为 $\{a^1,a^2,...,a^n\}$ ，如果取 $(a^i,a^{i+1})$ 的中点作为划分点，则总共可能有 n-1 个划分点：

$$T_a={\lbrace{\frac{a^i+a^{i+1}}{2}|1\leq{i}\leq{n-1}}\rbrace}$$

基于某一个划分点二分之后的信息增益为：

$$\begin{aligned}
\operatorname{Gain}(D,a) &= \max_{t\in{T_a}}\operatorname{Gain}(D,a,t)\\
&=
\max_{t\in{T_a}}\operatorname{Ent}(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_t^{\lambda}|}
{|D|}\operatorname{Ent}(D_t^{\lambda})
\end{aligned}$$

可以选择使得 $\operatorname{Gain}(D,a)$ 最大的划分点。

连续属性可以在后续继续作为划分属性。



## 多变量决策树

使用决策变量的线性组合 $\sum_{i=1}^d w_ia_i=t$ 进行分类

$w_i$ 和 $t$ 可以学习得到。



## python

### 调用sklearn

ref: https://scikit-learn.org/stable/modules/tree.html

```python




```

#### 打印树的结果：


![image-20220704113609984](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220704113609984.png)

以上树为例，结果如下：

其中，树结构主要靠children_left和children_right ，它们记录了左右节点编号

children_left[0] 代表 第0(根节点)个节点的左节点编号为1，同理，右节点编号为 children_right[0] = 4，
 -1代表没有子节点(即叶子节点)。

**1. sklearn 自带一个函数 tree.export_text(clf)**

```python

tree.export_text(clf3)

|--- feature_0 <= 0.50
|   |--- feature_1 <= 0.50
|   |   |--- class: False
|   |--- feature_1 >  0.50
|   |   |--- class: False
|--- feature_0 >  0.50
|   |--- feature_1 <= 0.50
|   |   |--- class: True
|   |--- feature_1 >  0.50
|   |   |--- class: True

```



**2. 自己写**

```python
def print_tree_detail(clf):
    """ 打印树的结构 """
    children_left    = clf.tree_.children_left            # 左节点编号
    children_right   = clf.tree_.children_right            # 右节点编号
    feature        = clf.tree_.feature               # 分割的变量
    threshold       = clf.tree_.threshold              # 分割阈值
    impurity       = clf.tree_.impurity               # 不纯度(gini)
    n_node_samples   = clf.tree_.n_node_samples            # 样本个数
    value         = clf.tree_.value                 # 样本分布
    
    #-------------打印------------------------------
    print("children_left:",children_left)        
    print("children_right:",children_right)
    print("feature:",feature)
    print("threshold:",threshold)
    print("impurity:",impurity)
    print("n_node_samples:",n_node_samples)
    print("value:",value)

——————————————————————————————result——————————————————————————————
children_left: [ 1  2 -1 -1  5 -1 -1]
children_right: [ 4  3 -1 -1  6 -1 -1]
feature: [ 0  1 -2 -2  1 -2 -2]
threshold: [ 0.5  0.5 -2.  -2.   0.5 -2.  -2. ]
impurity: [0.3143992  0.00116414 0.         0.00328946 0.41705546 0.39669421
 0.41771983]
n_node_samples: [2375 1717 1110  607  658   22  636]
value: [[[1.911e+03 4.640e+02]]
 [[1.716e+03 1.000e+00]]
 [[1.110e+03 0.000e+00]]
 [[6.060e+02 1.000e+00]]
 [[1.950e+02 4.630e+02]]
 [[6.000e+00 1.600e+01]]
 [[1.890e+02 4.470e+02]]]

```



#### 画P-R图

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html

https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py

```python
## --- 打印P-R图
def get_pr_display(clf,X_test):
    """ 根据X_test 预测 y_score(即概率),画P-R图 """
    y_score = clf.predict_proba(X_test)
    y_score = y_score[:,1]  # --- 预测为 1 的概率

    # n_classes = clf.tree_.n_classes[0]

    ## --- precision_recall_curve 返回值：precision, recall, thresholds
    prec, recall,_ = precision_recall_curve(y_test, y_score)
    average_precision = average_precision_score(y_test, y_score)
    logger.info("precision: {}, recall: {}, average_precision: {}".format(prec, recall, average_precision))
    pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()
    return pr_display
```





### sklearn 剪枝

#### 预剪枝

##### min_samples_leaf

限定一个结点在分支后的每个子结点都必须包含至少min_samples_ leaf个训练样本，否则分支就不会发生，或者，分支会朝着满足每个子结点都包含min\\\_samples\\\_ leaf个样本的方向去发生。 &#x20;
一般搭配max_depth使用，**在回归树中有神奇的效果**，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据⼀一般来说，建议从=5开始使用。如果叶结点中含有的样本量变化很 大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子结点出现。**对于类别不多的分类问题，=1通常就是最佳选择。**

##### min_samples_split

限定一个结点必须要包含至少min_samples_split个训练样本，这个结点才允许被分支，否则分支就不会发生。

```python
clf = tree.DecisionTreeClassifier(criterion="entropy",
                                  random_state=30,splitter="random"
                                    ,max_depth=3
                                    ,min_samples_leaf=10
                                    ,min_samples_split=10
                                    )
clf = clf.fit(Xtrain, Ytrain)
dot_data = tree.export_graphviz(clf
                                ,feature_names= feature_name
                                ,class_names=["琴酒","雪莉","⻉贝尔摩德"]
                                ,filled=True
                                ,rounded=True
                                )
graph = graphviz.Source(dot_data)
graph

```

运行结果 &#x20;

![](https://img-blog.csdnimg.cn/20191006224117569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dyYWNlanB3,size_16,color_FFFFFF,t_70)

```python
clf.score(Xtrain,Ytrain)

```

运行结果

```python
0.8306451612903226

```

```python
clf.score(Xtest,Ytest)

```

运行结果

```python
0.8518518518518519

```

##### max_ features

一般max_depth用作树的”精修“。 &#x20;
max_ features限制分支时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工， max_ features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以 &#x20;
使用的特征数量而强行使决策树停下的参数，**在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。**

##### min_impurity_decrease

**min_impurity_ decrease限制信息增益的大小，信息增益小于设定数值的分支不会发生**。这是在0.19版本中更新的功能，在0.19版本之前时使用min_impurity_ split。

##### 举例：用学习曲线来确定超参 max_depth

那么，具体如何来确定每个参数填写什么值呢？这时候，我们就要使用确定超参数的曲线来进行判断了，继续使用我们已经训练好的决策树模型clf。超参数的学习曲线，是一条以超参数的取值为横坐标，模型的度量指标为纵坐标的曲线， 是用来衡量不同超参数取值下模型的表现的线。在我们建好的决策树里，我们的模型度量指标就是score。

```python
test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(max_depth=i+1
                                      ,criterion="entropy"
                                      ,random_state=30
                                      ,splitter="random"
                                     )
    clf = clf.fit(Xtrain, Ytrain)
    score = clf.score(Xtest, Ytest)
    test.append(score)
plt.plot(range(1,11),test,color="red",label="max_depth")
plt.legend()
plt.show()

```

运行结果 &#x20;

![](https://img-blog.csdnimg.cn/20191006225112871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dyYWNlanB3,size_16,color_FFFFFF,t_70)

可见，max_depth=3是最优选择。 &#x20;

总之，无论如何，剪枝参数的默认值会让树无尽生长，这些树在某些数据集上可能非常大，对内存的消耗也非常大。 所以如果数据集非常大，已经预测到无论如何都是要剪枝的，那么，提前设定这些参数来控制树的复杂性和大小会比较好。



#### 后剪枝

references:

> [1.10.8. Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/modules/tree.html#id2)
> [post pruning decision trees with cost complexity pruning](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)

##### [1.10.8. Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/modules/tree.html#id2)

reference:  Chapter 3 of Breiman 1984

-  $α≥0$ ：**the complexity parameter**
- **cost-complexity measure**（也叫**子树的损失函数**）, $R_α(T)$ of a given tree T:

$Rα(T)=R(T)+α|\widetilde{T}|$

$|\widetilde{T}|$ 是树 T 中叶节点的个数。

$R(T)$ 原本为 misclassification rate of the terminal nodes，在sklearn中使用 weighted impurity of the terminal nodes。

> sklearn说inpurity有多种衡量方法，可能对于classification和regression各有不同，如MSE等

- **Minimal cost-complexity pruning** finds the subtree of T that minimizes $R_α(T)$.

  一个单一 node （该node作为叶子节点）的cost complexity measure 是  $R_α(t)=R(t)+α$。 该节点下的子树记为 $T_t$，一般而言，$R(T_t)<R(t)$。在 $\alpha$ 取特定值的时候，$R_α(T_t)=R_α(t)$，使之相等的 $\alpha$ 记为 $\alpha_{eff}(t)$ ，$α_{eff}(t)=\frac{R(t)−R(T_t)}{|T|−1}$。 $\alpha_{eff}(t)$ 最小的子树会被剪枝。

- **当**  $min{\alpha_{eff}(t)}$ > 参数 `ccp_alpha` 时，停止。



##### [post pruning decision trees with cost complexity pruning](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html) 基于CCP(cost complexity proning) 的后剪枝

cost complexity parameter ： `ccp_alpha` 。该参数越大，被剪掉的树越大。

```
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
```

###### Total impurity of leaves vs effective alphas of pruned tree

[`DecisionTreeClassifier.cost_complexity_pruning_path`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path)

> **cost_complexity_pruning_path**(*X*, *y*, *sample_weight=None*)[[source\]](https://github.com/scikit-learn/scikit-learn/blob/baf0ea25d/sklearn/tree/_classes.py#L607)

该函数返回所有的effective $\alpha$ ，以及所有 leaf （叶子节点）的impurity 之和 

```
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

clf = DecisionTreeClassifier(random_state=0)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
```

注：下图中去掉了最大的effective $\alpha$，即只有一个根节点的树（ccp_alphas[:-1], impurities[:-1]）

```
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
```

![Total Impurity vs effective alpha for training set](https://scikit-learn.org/stable/_images/sphx_glr_plot_cost_complexity_pruning_001.png)

Out:

```
Text(0.5, 1.0, 'Total Impurity vs effective alpha for training set')
```

如果在 DecisionTreeClassifier 训练的时候，加上 ccp_alpha 参数，那么可以训练出来一组决策树

 `clfs[-1]` ：对应一个node

```
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)
```

Out:

```
Number of nodes in the last tree is: 1 with ccp_alpha: 0.3272984419327777
```

剩余的决策树：当 ccp_alpha 增加，node数减少，树的深度也减少

```
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
```

![Number of nodes vs alpha, Depth vs alpha](https://scikit-learn.org/stable/_images/sphx_glr_plot_cost_complexity_pruning_002.png)

###### 训练集和测试集，在alpha不同的时候的准确率

alpha不断增大，展示了由过拟合到欠拟合的过程。当`ccp_alpha=0.015` 时，测试集准确率最高。

```
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()
```

![Accuracy vs alpha for training and testing sets](https://scikit-learn.org/stable/_images/sphx_glr_plot_cost_complexity_pruning_003.png)





### 画图

```python
import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("pdfname")
```

graphviz需要
1. pip install graphviz
2. 去官网下载.exe安装，并且把dot.exe加入path



示例：

```code
digraph Tree {
node [shape=box, fontname="helvetica"] ;
edge [fontname="helvetica"] ;
0 [label="X[0] <= 0.5\ngini = 0.314\nsamples = 2375\nvalue = [1911, 464]"] ;
1 [label="X[1] <= 0.5\ngini = 0.001\nsamples = 1717\nvalue = [1716, 1]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="gini = 0.0\nsamples = 1110\nvalue = [1110, 0]"] ;
1 -> 2 ;
3 [label="gini = 0.003\nsamples = 607\nvalue = [606, 1]"] ;
1 -> 3 ;
4 [label="X[1] <= 0.5\ngini = 0.417\nsamples = 658\nvalue = [195, 463]"] ;
0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
5 [label="gini = 0.397\nsamples = 22\nvalue = [6, 16]"] ;
4 -> 5 ;
6 [label="gini = 0.418\nsamples = 636\nvalue = [189, 447]"] ;
4 -> 6 ;
}
```



左支为 True

![image-20220704113609984](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220704113609984.png)










