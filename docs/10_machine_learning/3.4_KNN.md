[TOC]

##### 参考资料

sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

wiki: [k-nearest neighbors algorithm - Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)



### 基本原理

KNN(k-NearestNeighbor),就是k最近邻算法，这是一种常用的监督学习方法，简单来说，根据k个最近的邻居的状态来决定样本的状态，即‘物以类聚，人以群分’。



### 超参

① k：k的选择取决于数据。更大的k可以忽视noise的效果，但会导致边界难以区分。



## Python

### sklearn

https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

> 老版本：1.0.2 版本

https://scikit-learn.org/1.0/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=neighbor#sklearn.neighbors.KNeighborsClassifier

[1.6. Nearest Neighbors — scikit-learn 1.3.0 documentation](https://scikit-learn.org/stable/modules/neighbors.html#classification)





#### 参数

- p：minkowski距离函数的参数，默认=2为欧氏距离，=1表示曼哈顿距离，=∞ 时，闵氏距离 为 切比雪夫距离

- weight：表示是否根据半径内的距离不同给更高的权重。如果是uniform就都一样，如果weights = 'distance'就是更近的权重更高。weight = 1/distance. **（这个和imbalanced data 没有关系）**



## 怎么做imbanlanced data

### knn -- imbalanced data -- cost sensitive 

比较靠谱的是这篇（虽然觉得效果不一定好）：

> 2003_PRL_Choosing k for two-class nearest neighbour classifiers with unbalanced classes

> 这篇是2018那本书推荐的，fermandez

（主要是其他的没有找到更靠谱的）

假设 $c_0$ 表示把class 0 错误归类为 class 1 的成本； $c_1$ 类似

那么当 $p(1|x)>c_0/(c_0+c_1)$ 时，将该类别归类为1。 （其中 p(1|x) 就是邻居中1的数量/邻居的总数量）

 

**现象：**很明显，如果这么归类的话，那么当 $k\le (1-c_0)^{-1}$ 时 （已经将$c_0+c_1=1$ 归一化），除非所有的neighbor都为1，才会为1 （在这里我们假设$c_0>>c_1$）

那么很显然，当 $k$ 增加时，class 1 被分错的概率也是递增的。

在每个 $(1-c_0)^{-1}$ 的分界点上，这个概率又会骤跌，所以会呈现锯齿形。







### knn -- imbalanced data -- under sampling是有文章的

Mani, I., & Zhang, I. (2003, August). kNN approach to unbalanced data distributions: a case study involving information extraction. In *Proceedings of workshop on learning from imbalanced datasets* (Vol. 126, No. 1, pp. 1-7). ICML.













#### 总觉得knn不适合用于imbalanced data（cost sensitive）或者说效果会不好



> Sun, Y., Kamel, M. S., Wong, A. K., & Wang, Y. (2007). Cost-sensitive boosting for classification of imbalanced data. *Pattern recognition*, *40*(12), 3358-3378.

该文也提到说不怎么好用

![image-20230702205805414](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230702205805414.png)