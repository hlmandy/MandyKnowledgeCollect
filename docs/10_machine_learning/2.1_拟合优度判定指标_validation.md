[TOC]

## confusion matrix 混淆矩阵

https://en.wikipedia.org/wiki/Confusion_matrix

![image-20220703162403920](C:\Users\mandy\AppData\Roaming\Typora\typora-user-images\image-20220703162403920.png)

### 一系列术语

- [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_(test)), [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall), [hit rate](https://en.wikipedia.org/wiki/Hit_rate), or [true positive rate](https://en.wikipedia.org/wiki/Sensitivity_(test)) (TPR)  --- 查全率

  $$\mathrm {TPR} ={\frac {\mathrm {TP} }{\mathrm {P} }}={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FN} }}=1-\mathrm {FNR}$$

- [specificity](https://en.wikipedia.org/wiki/Specificity_(tests)), [selectivity](https://en.wikipedia.org/wiki/Specificity_(tests)) or [true negative rate](https://en.wikipedia.org/wiki/Specificity_(tests)) (TNR)

  $$ \mathrm {TNR} ={\frac {\mathrm {TN} }{\mathrm {N} }}={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FP} }}=1-\mathrm {FPR} $$

- [precision](https://en.wikipedia.org/wiki/Information_retrieval#Precision) or [positive predictive value](https://en.wikipedia.org/wiki/Positive_predictive_value) (PPV)  --- 查准率

  $$ \mathrm {PPV} ={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FP} }}=1-\mathrm {FDR}$$

- [negative predictive value](https://en.wikipedia.org/wiki/Negative_predictive_value) (NPV)

  $$\mathrm {NPV} ={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FN} }}=1-\mathrm {FOR}$$

- miss rate or [false negative rate](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#False_positive_and_false_negative_rates) (FNR)

  $$\mathrm {FNR} ={\frac {\mathrm {FN} }{\mathrm {P} }}={\frac {\mathrm {FN} }{\mathrm {FN} +\mathrm {TP} }}=1-\mathrm {TPR}$$

- [fall-out](https://en.wikipedia.org/wiki/False_positive_rate) or [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate) (FPR)

  $$\mathrm {FPR} ={\frac {\mathrm {FP} }{\mathrm {N} }}={\frac {\mathrm {FP} }{\mathrm {FP} +\mathrm {TN} }}=1-\mathrm {TNR}$$

- [false discovery rate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR)  假正率  false positive rate

  $$\mathrm {FDR} ={\frac {\mathrm {FP} }{\mathrm {FP} +\mathrm {TP} }}=1-\mathrm {PPV}$$

- [false omission rate](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values) (FOR)

  $$\mathrm {FOR} ={\frac {\mathrm {FN} }{\mathrm {FN} +\mathrm {TN} }}=1-\mathrm {NPV}$$

- [Positive likelihood ratio](https://en.wikipedia.org/wiki/Positive_likelihood_ratio) (LR+)

  $${\displaystyle \mathrm {LR+} ={\frac {\mathrm {TPR} }{\mathrm {FPR} }}}$$

- [Negative likelihood ratio](https://en.wikipedia.org/wiki/Negative_likelihood_ratio) (LR-)

  $${\displaystyle \mathrm {LR-} ={\frac {\mathrm {FNR} }{\mathrm {TNR} }}}$$

- [prevalence threshold](https://en.wikipedia.org/wiki/Prevalence_threshold) (PT)

  $${\displaystyle \mathrm {PT} ={\frac {{\sqrt {\mathrm {TPR} (-\mathrm {TNR} +1)}}+\mathrm {TNR} -1}{(\mathrm {TPR} +\mathrm {TNR} -1)}}={\frac {\sqrt {\mathrm {FPR} }}{{\sqrt {\mathrm {TPR} }}+{\sqrt {\mathrm {FPR} }}}}}$$

- threat score (TS) or critical success index (CSI)

  $${\displaystyle \mathrm {TS} ={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FN} +\mathrm {FP} }}}$$
  
   

------

- [Prevalence](https://en.wikipedia.org/wiki/Prevalence)

  $${\displaystyle {\frac {\mathrm {P} }{\mathrm {P} +\mathrm {N} }}}$$

- [accuracy](https://en.wikipedia.org/wiki/Accuracy) (ACC)

  $${\displaystyle \mathrm {ACC} ={\frac {\mathrm {TP} +\mathrm {TN} }{\mathrm {P} +\mathrm {N} }}={\frac {\mathrm {TP} +\mathrm {TN} }{\mathrm {TP} +\mathrm {TN} +\mathrm {FP} +\mathrm {FN} }}}$$

- balanced accuracy (BA)

  $${\displaystyle \mathrm {BA} ={\frac {TPR+TNR}{2}}}$$

- [F1 score](https://en.wikipedia.org/wiki/F1_score)

  is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean#Harmonic_mean_of_two_numbers) of [precision](https://en.wikipedia.org/wiki/Information_retrieval#Precision) and [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_(test)):
  $${\displaystyle \mathrm {F} _{1}=2\times {\frac {\mathrm {PPV} \times \mathrm {TPR} }{\mathrm {PPV} +\mathrm {TPR} }}={\frac {2\mathrm {TP} }{2\mathrm {TP} +\mathrm {FP} +\mathrm {FN} }}}$$
  
- [phi coefficient](https://en.wikipedia.org/wiki/Phi_coefficient) (φ or rφ) or [Matthews correlation coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) (MCC)

  $${\displaystyle \mathrm {MCC} ={\frac {\mathrm {TP} \times \mathrm {TN} -\mathrm {FP} \times \mathrm {FN} }{\sqrt {(\mathrm {TP} +\mathrm {FP} )(\mathrm {TP} +\mathrm {FN} )(\mathrm {TN} +\mathrm {FP} )(\mathrm {TN} +\mathrm {FN} )}}}}$$

- [Fowlkes–Mallows index](https://en.wikipedia.org/wiki/Fowlkes–Mallows_index) (FM)

  $${\displaystyle \mathrm {FM} ={\sqrt {{\frac {TP}{TP+FP}}\times {\frac {TP}{TP+FN}}}}={\sqrt {PPV\times TPR}}}$$

- [informedness](https://en.wikipedia.org/wiki/Informedness) or bookmaker informedness (BM)

  $${\displaystyle \mathrm {BM} =\mathrm {TPR} +\mathrm {TNR} -1}$$

- [markedness](https://en.wikipedia.org/wiki/Markedness) (MK) or deltaP (Δp)

  $${\displaystyle \mathrm {MK} =\mathrm {PPV} +\mathrm {NPV} -1}$$

- [Diagnostic odds ratio](https://en.wikipedia.org/wiki/Diagnostic_odds_ratio) (DOR)

  $${\displaystyle \mathrm {DOR} ={\frac {\mathrm {LR+} }{\mathrm {LR-} }}}$$



![image-20220703163425078](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220703163425078.png)

### P-R 曲线

- Precision：查准率  $\frac{TP}{TP+FP}$   ，即预测为True的集合中，实际为True的比例；
- Recall：查全率 $\frac{TP}{TP+FN}$，即实际为True的集合中，预测为True的比例；

一般而言，查准率越高，查全率越低；查全率越高，查准率越低。

#### P-R曲线绘制方法：

将所有样本按照预测的可能性由高至低排序，然后就可以画出P-R图。

> 当只将1个样本判定为True时，查全率接近0，查准率接近1；
>
> 当判定为True的样本越来越多，查全率越来越大，而查准率越来越低；

<img src="https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220703170721946.png" alt="image-20220703170721946" style="zoom:67%;" />

从图2.3中可以看出，因为学习器A包住了学习器C，所以学习器A比C好。

但是A和B无法判断，需要综合考虑的度量

如：

- BEP：Break-Event Point，“平衡点” 它是 “查 准率=查全率” 时的取值

  缺点：过于简化

-  $F 1$ 度量:
$$
F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N} .
$$

由于不同场景下，对于查准率和查全率的重视程度有所不同，可以使用  $F 1$ 度量的一般形式 $F_{\beta}$，
$$
F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R},
$$

$\beta$ 体现了对查准率/查全率的不同偏好。

> harmonic mean （谐波）
> $F 1$ 是基于查准率与查 全率的调和平均(harmonic mean)定义的: 
> $$
> \frac{1}{F 1}=\frac{1}{2} \cdot\left(\frac{1}{P}+\frac{1}{R}\right) \text {. }
> $$
> $F_{\beta}$ 则是加权调和平均:
> $$
> \frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right) .
> $$
> 与算术平均 $\left(\frac{P+R}{2}\right)$ 和几 何平均 $(\sqrt{P \times R})$ 相比, 调 和平均更重视较小值.
其中 $\beta>0$ 度量了查全率对查准率的相对重要性 [Van Rijsbergen, 1979]. $\beta=1$ 时退化为标准的 $F 1 $;

**$ \beta>1$ 时查全率有更大影响; $\beta<1$ 时查准率有更大影响.**

#### 多个混淆矩阵时如何综合考察查准率和查全率

我们希望在 $n$ 个二分类混淆矩阵上综合考察查准率和查全率.
一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率, 记为 $\left(P_{1}, R_{1}\right),\left(P_{2}, R_{2}\right), \ldots,\left(P_{n}, R_{n}\right)$, 再计算平均值, 这样就得到 “宏查准 率” $($ macro- $P)$ 、“宏查全率” (macro- $R)$, 以及相应的 “宏 $F 1$ ” (macro- $F 1)$ :
$$
\begin{gathered}
\text { macro- } P=\frac{1}{n} \sum_{i=1}^{n} P_{i}, \\
\text { macro- } R=\frac{1}{n} \sum_{i=1}^{n} R_{i}, \\
\text { macro- } F 1=\frac{2 \times \text { macro- } P \times \text { macro- } R}{\text { macro- } P+\text { macro- } R} .
\end{gathered}
$$
还可先将各混淆矩阵的对应元素进行平均, 得到 $T P 、 F P 、 T N 、 F N$ 的 平均值, 分别记为 $\overline{T P} 、 \overline{F P} 、 \overline{T N} 、 \overline{F N}$, 再基于这些平均值计算出 “微查准 率” $($ micro- $P)$ 、“微查全率” (micro- $R)$ 和 “微 $F 1$ ” (micro-F1):

micro- $P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}$,
micro- $R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}$,
micro- $F 1=\frac{2 \times \text { micro- } P \times \text { micro- } R}{\text { micro- } P+\text { micro- } R} .$






#### discriminatory performance (DP)

- sensitivity : 认定为真/实际为真=recall
- specificity : 认定为假/实际为假

在不同的cut-of point 下，Se/Sp 不同

举例，当$c_p=0.2$，

![image-20220410202027359](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220410202027359.png)

$Se = 70/100=0.7$

$Sp = 80/100 = 0.8$



Perfect (ideal) discrimination: $Se = Sp = 1$

对于给定的cut-point，$Se$，$Sp$ 越大，DP 越好

> Example: cut-point = 0.2:
>
> Model 1: Se = 0.7 and Sp = 0.8
>
> better DP than
>
> Model 2: Se = 0.6 and Sp = 0.5

#### ROC曲线 receiver operating characteristic

对于逻辑回归而言，ROC就是取不同的cut point时，$\frac{Se}{1-Sp}$ 的图形。

注意，当cut point = 0 时，必全部判定为真，cp = 0: Se = 1, Sp = 0，在（1,1）点；当cut point = 1 时，必全部判定为假，cp = 1: Se = 0, Sp = 1在（0,0）点；

When applied to a logistic model, an ROC is a plot of sensitivity (Se) vs. 1 - specificity (1 - Sp)

derived from several cut-points for the predicted value.

![image-20220417103136133](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220417103136133.png)



> 有些图横轴不用1-se，因此坐标轴不是从0到1，而是从1到0，如下所示

![image-20220410152602449](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220410152602449.png)

> 45°线相当于随机选的结果。所以曲线应当高于45°，表示模型的预测效果。也表示真阳性率>假阳性率
>
> Model 1: TPR $\ge$ FPR always : Excellent discrimination
>
> Model 2: TPR = FPR always : No discrimination



#### AUC

ROC曲线下面的面积叫做AUC（area under an ROC curve ），也叫做 c-statistic。

> 为什么AUC面积越大越好：我们希望实际为真的那些case比实际为假的那些case有更高的概率被预测为真。

> 0.90–1.0 = excellent discrimination (A)
>
> 0.80–0.90 = good discrimination(B)
>
> 0.70–0.80 = fair discrimination (C)
>
> 0.60–0.70 = poor discrimination(D)
>
> 0.50–0.60 = failed discrimination(F)

**AUC计算公式：**

我们现在样本里有100真，200假。随机选一真一假组合，有200 * 100种可能性。在这些组合里，Pr(真)>Pr(假)概率为 57.4%；Pr(真)=Pr(假)概率为 27.1%

则AUC = 57.4% + 27.1% * 0.5 =  0.7095

![image-20220417111426668](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20220417111426668.png)





#### AIC/BIC

$AIC=−2×L+2×(p+1)$

 





### Lift curve

> https://zhuanlan.zhihu.com/p/401424937

```text
Precision = TP/(TP+FP)
PP（percentage of positive samples）= TP+FN/(TP+FP+TN+FN)
Lift = precision/PP
x 轴： Depth = (TP + FP)/(TP+FP+TN+FN) 
```

`Precision`和的比率`PP`是提升值，它衡量与不使用模型相比，模型在预测方面“更好”的程度。



## sklearn metrics

https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics



