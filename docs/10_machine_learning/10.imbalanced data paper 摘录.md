[TOC]

## He, H., & Garcia, E. A. (2009). Learning from imbalanced data. *IEEE Transactions on knowledge and data engineering*, *21*(9), 1263-1284.

高引用

> 总结：1）直接看PR图是最好的；2）如果要一个综合的统计指标，可以考虑 $F_\beta$

### 4. 评估指标

![image-20230703140135849](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230703140135849.png)

#### 4.1 singular评估指标

**第1组：Accuracy**

$$Accuracy = \frac{TP+TN}{P_C+N_C}, ErrorRate = 1 - accuracy$$

用到了混淆矩阵左右两列的信息，因此对数据分布异常敏感。在imbalance data 情形下，不好用！

在存在不平衡数据的情况下，当评估指标对数据分布敏感时，很难进行相对分析。



**第2组：**

> 论文中 F-Measure写错了，下面已经更正；
>
> F-Measure 是一种 harmonic mean

$$
\begin{gathered}
\text { Precision }=\frac{T P}{T P+F P}, \\
\text { Recall }=\frac{T P}{T P+F N}, \\
\text { F-Measure }=\frac{(1+\beta^2) \cdot \text { Recall } \cdot \text { Precision }}{\beta^2 \cdot \text { Recall }+ \text { Precision }},
\end{gathered}
$$
where $\beta$ is a coefficient to adjust the relative importance of precision versus recall (usually, $\beta=1$ ):
$$
G \text {-mean }=\sqrt{\frac{T P}{T P+F N} \times \frac{T N}{T N+F P}} .
$$


1. recall 查全率，precision 查准率，precision对数据分布敏感，recall对数据分布不敏感

   但是仅依赖于recall不能知道有多少0被误标记为1。

   因此最好将两个方法合并起来，如F和G

2. 但该文认为，尽管F和G比Accuracy更好，但还不够好。

> hl的理解是：仅知道均值，但不知道分布（就像ROC是基于分布的积分）



##### 引申

Chawla, N. V., Cieslak, D. A., Hall, L. O., & Joshi, A. (2008). Automatically countering imbalance and its empirical relationship to cost. *Data Mining and Knowledge Discovery*, *17*, 225-252.

这篇文章说，直接用 $F_\beta$， $\beta$ 就是cost matrix比值。但是这篇文章的引用不高，只有300+

2.4.3 $\beta$ varied $f$-measure
Another wrapper evaluation criterion is based on an alternate derivation of the previously outlined $f$-measure. Here, a separate $\beta$ factor is incorporated to introduce an element of relative importance between recall and precision:
$$
f \text { measure }_\beta=\frac{\left(1+\beta^2\right) \times \text { precision } \times \text { recall }}{\beta^2 \times \text { recall }+ \text { precision }}
$$
As $\beta$ decreases, the importance of precision diminishes. To adapt this behavior to a cost-based framework the $\beta$ value must depend on the cost ratio. Therefore, $\beta$ will be set as:
$$
\beta=\frac{C(+\mid-)}{C(-\mid+)}
$$

> 解释：
> $$
> f \text { measure }_\beta=\frac{(1+\frac{1}{\beta^2}) }{\frac{1}{\beta^2 \times \text { recall }}+ \frac{1}{\text { precision }}}
> $$
> 可以看出来，$\beta$ 越大，precision越重要。
>
> >  注意，这个公式可以是反的，wiki上就是反的，$\beta$ 越大，recall越重要。[F-score - Wikipedia](https://en.wikipedia.org/wiki/F-score)
>
> C(-|+)表示把+的预测为-的。在本文中，+例稀少，比较重要，则 C(+|-) = 1， C(-|+) = IR。$\beta<1$，表示recall更重要，查全率更重要，把所有的1都找出来比较重要。





> 一个review（citation 500+）介绍了这个指标：Bekkar, M., Djemaa, H. K., & Alitouche, T. A. (2013). Evaluation measures for models assessment over imbalanced data sets. *J Inf Eng Appl*, *3*(10).
>
> 这篇review不要引，错误百出，太差了。



#### 4.2 ROC

$$
\text { TP_rate }=\frac{T P}{P_C} ; \quad F P \_ \text {rate }=\frac{F P}{N_C} .
$$



在分布有偏的数据集中，ROC曲线过于乐观了，PR曲线更好

#### 4.3 Precision-Recall （PR）Curves

该文提出要参考 2006_The Relationship between Precision Recall and ROC Curves_PR曲线高引用

2006这篇指出 ROC 和PR 曲线一一对应 混淆矩阵，

ROC曲线dominance的，PR曲线也dominance

同时AUC-ROC 最大化不等于 AUC-PR最大化。



![image-20230703152345695](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20230703152345695.png)

> Fernandez_2018_Book_LearningFromImbalancedDataSets 书中画的PR图表明，PR图不是单调的，且与AUC-ROC含义不同。
>
> As in the ROC case, the *AUC**P R* can be used as a summary statistic for the PR graph. *AUC**P R* does not have a probabilistic interpretation as *AUC**ROC* does. The *AUC**P R* of the random classififier varies with the prevalence of the positive class, and its expected value is close to the proportion of positive instances in the test set.
>
> 并且，直接看PR图和ROC图永远是最好的。













