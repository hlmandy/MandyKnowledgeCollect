RNN循环神经网络 & CNN卷积神经网络 处理时间序列

ref: 

1. deep learning for time series forecasting
2. 机器学习实战

[TOC]

# CNN

# RNN

循环神经网络（RNN），这是一类可以预测未来的网络（当然，是一定程度上）。它们可以分析时间序列数据（例如股票价格），并告诉你何时进行买卖。  

RNN具有一定的记忆性

## 循环神经元和层

![image-20211012101145639](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012101145639.png)

图：单个神经元的展开

![image-20211012101145639](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012101145639.png)

图：单层神经元的展开



你可以轻松地创建一层递归神经元。在每个时间步长t，每个神经元接受输入向量x（t）和前一个时间步长的输出向量y（t-1） 。

Equation 15-1. Output of a recurrent layer for a single instance
$$
\mathbf{y}_{(t)}=\phi\left(\mathbf{W}_{x}^{\top} \mathbf{x}_{(t)}+\mathbf{W}_{y}^{\top} \mathbf{y}_{(t-1)}+\mathbf{b}\right)
$$
Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a mini-batch
$$
\begin{aligned}
\mathbf{Y}_{(t)} &=\phi\left(\mathbf{X}_{(t)} \mathbf{W}_{x}+\mathbf{Y}_{(t-1)} \mathbf{W}_{y}+\mathbf{b}\right) \\
&=\phi\left(\left[\begin{array}{ll}
\mathbf{X}_{(t)} & \mathbf{Y}_{(t-1)}
\end{array}\right] \mathbf{W}+\mathbf{b}\right) \text { with } \mathbf{W}=\left[\begin{array}{l}
\mathbf{W}_{x} \\
\mathbf{W}_{y}
\end{array}\right]
\end{aligned}
$$
记忆单元 Memory Cells

单个循环神经元或一层循环神经元是一个非常基本的单元，它只能学习短模式 。

![image-20211012110603126](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012110603126.png)

Figure 15-3. A cell’s hidden state and its output may be different



下图为四种输入-输出序列

![image-20211012110828113](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012110828113.png)

Figure 15-4. Seq-to-seq (top left), seq-to-vector (top right), vector-to-seq (bottom left), and  Encoder–Decoder (bottom right) networks



## 训练RNN

要训练RNN，诀窍是将其按照时间逐步展开（就像我们刚才所做的那样），然后简单地使用常规的反向传播（见图15-5）。这种策略称为“时间反向传播”（BackPropagation Through Time，BPTT）。  



![image-20211012111418536](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012111418536.png)

## 预测时间序列



```python
# 单个神经元单层RNN
model = keras.models.Sequential([
	keras.layers.SimpleRNN(1, input_shape=[None, 1])
])
```

默认情况下，SimpleRNN层使用双曲正切激活函数。  



```python
def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise
    return series[..., np.newaxis].astype(np.float32)

n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]

# baseline
y_pred = X_valid[:, -1]
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
#0.020211367

model = keras.models.Sequential([
    keras.layers.SimpleRNN(1, input_shape=[None, 1])
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_test, y_test,epochs=20)
y_pred = model.predict(X_valid)
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))

```

**趋势和季节性：**

使用RNN时，通常不需要执行所有这些操作，但是在某些情况下可能会提高性能，因为该模型不需要学习趋势或季节性。  

### 深度RNN

![image-20211012112909704](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012112909704.png)

使用tf.keras实现深度RNN非常简单：只需堆叠循环层。  





## 处理长序列 Handling Long Sequences

长序列的网络较深，梯度不稳定，会逐渐忘记第一个输入

非饱和激活函数，也会导致梯度爆炸。

批量归一化不能像深度前馈网络那样高效地用于RNN。  

归一化的另一种形式通常与RNN一起使用会更好：层归一化。  



### 解决短期记忆问题

#### 长短期记忆（Long Short-Term Memory LSTM）单元  

![image-20211012165831605](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012165831605.png)

***Figure 15-9. LSTM cell***

它的状态被分为两个向量：h（t）和c（t）（“c”代表“单元cell”。你可以将h（t）视为短期状态，c（t）为长期状态。  当前输入为 $x_{(t)}$

你可以看到它首先经过一个遗忘门Forget gate，丢掉了一些记忆，然后通过加法操作添加了一些新的记忆（由输入门input gate  选择的记忆）  

在加法运算之后，长期状态被复制并通过tanh函数传输，然后结果被输出门output gate滤波。

- 主要层是 $g_t$​ 层，分析当前输入x（t）和先前（短期）状态h（t-1）  。在basic cell模型中，它的输出直接到y（t）和h（t）  ；在LSTM中，它的输出中的重要部分存储在长期状态中（其余部分则丢弃） 。
- 其他三层是门控制器 gate controllers  ，是 logistic 函数，输出范围(0,1)。它们的输出被馈送到逐元素乘法运算，因此，如果它们输出0，则关闭门；如果它们输出1，则将门打开。特别地 ：
  - 遗忘门（由f（t）控制）控制长期状态的哪些部分应当被删除。
  - 输入门（由i（t）控制）控制应将g（t）的哪些部分添加到长期状态。
  - 输出门（由o（t）控制）控制应在此时间步长读取长期状态的哪些部分并输出到h（t）和y（t）。  

简而言之，LSTM单元可以学会识别重要的输入（这是输入门的作用），将其存储在长期状态中，只要需要就保留它（即遗忘门的作用），并在需要时将其提取出来。  

#### GRU 门控循环单元, Gated Recurrent Unit  

是LSTM单元的简化版。

![image-20211012172457682](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211012172457682.png)









