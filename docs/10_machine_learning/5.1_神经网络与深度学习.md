[TOC]

**一些概念：**

神经网络：neural networks；神经元：neuron；激活函数：activation functions  



# 神经网络概述

![image-20211009132256946](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009132256946.png)

ref：周志华《机器学习》



**激活函数：**

![image-20211009132445106](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009132445106.png)

注：阶跃函数： step functions。周志华这里的step functions中的sgn疑有误，参考下图：



![image-20211009144009814](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009144009814.png)

ref: 机器学习实战2nd



人工神经元：artificial neuron  

![image-20211009144710628](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009144710628.png)



# 感知机 Perceptron

5.2感知机与多层网络

感知机(Perceptron )由两层神经元组成,如图5.3所示,输入层接收外界输入信号后传递给输出层,输出层是M-P神经元,亦称 threshold logic unit (TLU), or sometimes a linear threshold unit (LTU)  。

输入和输出是数字（而不是二进制开/关值），并且每个输入连接都与权重相关联。TLU计算其输入的加权和（z=w1x1+w2x2+…+wnxn=xTw），然后将阶跃函数应用于该和并输出结果：hw（x）=step（z），其中z=xTw。  

> 注：感知机用的是阶跃函数 step functions

![image-20211009144801604](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009144801604.png)



感知机能容易地实现逻辑与、或、非运算. 注意到 $y=f\left(\sum_{i} w_{i} x_{i}-\theta\right)$, 假 定 $f$ 是图 $5.2$ 中的阶跃函数, 有
- "与" $\left(x_{1} \wedge x_{2}\right)$ : 令 $w_{1}=w_{2}=1, \theta=2$, 则 $y=f\left(1 \cdot x_{1}+1 \cdot x_{2}-2\right)$, 仅在 $x_{1}=x_{2}=1$ 时, $y=1$;
- “或” $\left(x_{1} \vee x_{2}\right)$ : 令 $w_{1}=w_{2}=1, \theta=0.5$, 则 $y=f\left(1 \cdot x_{1}+1 \cdot x_{2}-0.5\right)$, 当 $x_{1}=1$ 或 $x_{2}=1$ 时, $y=1$;
- "非” $\left(\neg x_{1}\right)$ : 令 $w_{1}=-0.6, w_{2}=0, \theta=-0.5$, 则 $y=f\left(-0.6 \cdot x_{1}+0\right.$. $\left.x_{2}+0.5\right)$, 当 $x_{1}=1$ 时, $y=0$; 当 $x_{1}=0$ 时, $y=1$.

==这些都是线性可分问题：存在线性超平面将它们分开==

更一般地,给定训练数据集,权重$w_i$以及阈值 $\theta$ 可通过学习得到。阈值 $\theta$ 可看作一个固定输入为-1.0的“哑结点”(dummy node )所对应的连接权重Wn+1,这样,权重和阈值的学习就可统一为权重的学习感知机。
$$
\begin{gathered}
w_{i} \leftarrow w_{i}+\Delta w_{i} \\
\Delta w_{i}=\eta(y-\hat{y}) x_{i}
\end{gathered}
$$
==其中 $\eta \in(0,1)$ 称为学习率(learning rate).== 从式(5.1) 可看出, 若感知机对训练 样例 $(\boldsymbol{x}, y)$ 预测正确, 即 $\hat{y}=y$, 则感知机不发生变化, 否则将根据错误的程度 进行权重调整。



非线性可分问题，需要考虑多层功能神经元。

![image-20211009135057793](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009135057793.png)



前馈神经网络：feedforward neural network，指网络拓扑结构上不存在环或者回路。

## BP--误差逆传播 error BackPropagation，简称BP

反向传播算法能够针对每个模型参数计算网络误差的梯度。换句话说，它可以找出应如何调整每个连接权重和每个偏置项以减少误差。一旦获得了这些梯度，它便会执行常规的梯度下降步骤，然后重复整个过程，直到网络收敛到解。  



![image-20211009141530196](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009141530196.png)



输入层：$d$ 个

隐层：$q$ 个

输出层：$l$ 个

需要训练的权重：$v_{ih}$ 和 $w_{hj}$，以及阈值 $\theta_j$ 和 $\gamma_h$ 

目标：最小化均方误差

$E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}$

方法：梯度下降

**为什么叫反向传播：** 先将input向前传给输入层、隐层、输出层；然后计算误差，再将误差逆向传给隐层，调整$w$ 和 $\theta$；直到到达停止条件。

**如何设置隐层神经元的层数：**试错法

如何防止BP网络过拟合？

1. 早停：训练集误差降低但验证集误差升高时；
2. 正则化 regularization，

**随机初始化所有隐藏层的连接权重很重要，否则训练将失败，所有隐藏层神经元将一样！**

在BP中，将阶跃函数替换为Sigmod函数 σ（z）=1/（1+exp（-z））  

其他常用的激活函数包括：

- hyperbolic tangent function: tanh(z) = 2σ(2z) – 1  

- Rectified Linear Unit function: ReLU(z) = max(0, z)  

ReLU函数是连续的，但不幸的是，在z=0时，该函数不可微分（斜率会突然变化，这可能使梯度下降反弹），如果z<0则其导数为0。但是，实际上它运行良好并且具有计算快速的优点，因此它已成为默认值[11]。最重要的是，它没有最大输出值这一事实有助于减少梯度下降期间的某些问题。



![image-20211009150205830](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211009150205830.png)

***Figure 10-8. Activation functions and their derivatives*** （导数）



# 深度神经网络

如果你需要解决一个复杂的问题，例如检测高分辨率图像中的数百种物体，该怎么办？你可能需要训练更深的DNN，也许10层或更多层，每层包含数百个神经元，有成千上万个连接。训练深度DNN并不是在公园里散步。以下是你可能会遇到的一些问题：  

- 梯度消失与梯度爆炸
- 训练数据不足
- 训练缓慢
- 过拟合风险



## 梯度消失与梯度爆炸

**深度神经网络DNN的梯度不稳定**

- vanishing gradients  梯度消失：随着算法向下传播到较低层，梯度通常会越来越小。结果梯度下降更新使较低层的连接权重保持不变，训练不能收敛到一个良好的解。我们称其为梯度消失问题。
- exploding gradient   梯度爆炸：在某些情况下，可能会出现相反的情况：梯度可能会越来越大，各层需要更新很大的权重直到算法发散为止。这是梯度爆炸问题。



例如sigmod函数，当输入变大（负数或正数）时，该函数会以0或1饱和，并且导数非常接近0。因此反向传播开始时它几乎没有梯度可以通过网络传播回去。  

![image-20211011160101737](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011160101737.png)

***Figure 11-1. Logistic activation function saturation***



### 解决方法1

Glorot和Bengio  指出，需要信号在两个方向上正确流动，作者认为，我们需要每层输出的方差等于其输入的方差，并且我们需要在反方向时流过某层之前和之后的梯度具有相同的方差  。Glorot和Bengio提出了一个很好的折中方案，在实践中证明很好地发挥作用：

必须按照公式11-1中所述的随机初始化每层的连接权重，其中$fan_{avg}=（fan_{in}+fan_{out}）/2$。

| 初始化 | 激活函数                        | $\sigma^2$(Normal) |
| :----- | :------------------------------ | :----------------- |
| Glorot | None tanh 、logistic 、 softmax | $1/fan_{avg}$      |
| He     | ReLU 和变体                     | $2/fan_{in}$       |
| LeCun  | SELU                            | $1/fan_{in}$       |



默认情况下，Keras使用具有均匀分布的Glorot初始化。创建层时，可以通过设置kernel_initializer="he_uniform"或kernel_initializer="he_normal"来将其更改为He初始化：  

### 解决方法2--非饱和激活函数 Nonsaturating Activation Functions  

dying ReLUs  ：ReLU函数的输入为负时其梯度为零，因此对于训练集中的所有实例均为负数，神经元会死亡。

可以使用  ReLU函数的变体，例如leaky ReLU。该函数定义为LeakyReLUα（z）=max（αz，z）在z<0时有一个小的斜率，确保不会死亡。

![image-20211011154722390](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011154722390.png)

***Figure 11-2. Leaky ReLU: like ReLU, but with a small slope for negative values***  



它们可能会陷入长时间的昏迷，但是有机会最后醒来。  实际上，设置α=0.2（大泄漏）似乎比α=0.01（小泄漏）会产生更好的性能。  

 **$\alpha$ 可以学习得到**

参数化leaky ReLU（PReLU），其中α可以在训练期间学习（不是超参数，它像其他任何参数一样，可以通过反向传播进行修改）  

PReLU在大型图像数据集上的性能明显优于ReLU，但是在较小的数据集上，它存在过拟合训练集的风险。  



Djork-ArnéClevert等人在2015年发表的论文提出了一种新的激活函数[6]，称为指数线性单位（Exponential Linear Unit，ELU），该函数在作者的实验中胜过所有ReLU变体：减少训练时间，神经网络在测试集上表现更好。  



$\operatorname{ELU}_{\alpha}(z)= \begin{cases}\alpha(\exp (z)-1) & \text { if } z<0 \\ z & \text { if } z \geq 0\end{cases}$



![image-20211011154803715](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011154803715.png)

***Figure 11-3. ELU activation function***  



- 当z<0时，它取负值，这使该单元的平均输出接近于0，有助于缓解梯度消失的问题。超参数α定义一个值，该值为当z为较大负数时ELU函数逼近的值。通常将其设置为1，但是你可以像其他任何超参数一样对其进行调整。
- 对于z<0，它具有非零梯度，从而避免了神经元死亡的问题。
- 如果α等于1，则该函数在所有位置（包括z=0左右）都是平滑的，这有助于加速梯度下降，因为它在z=0的左右两侧弹跳不大。

ELU激活函数的主要缺点是它的计算比ReLU函数及其变体要慢（由于使用了指数函数）。它在训练过程中更快的收敛速度弥补了这种缓慢的计算，但是在测试时，ELU网络将比ReLU网络慢。  

然后，Günter Klambauer等人在2017年发表的论文[7]提出了可扩展的ELU（Scaled ELU，SEIU）激活函数。



尽管你的目标会有所不同，但通常SELU>ELU>leaky ReLU（及其变体）>ReLU>tanh>logistic  

```python
# leak ReLU
model = keras.models.Sequential([
    [...]
    keras.layers.Dense(10, kernel_initializer="he_normal"),
    keras.layers.LeakyReLU(alpha=0.2),
    [...]
])
# PReLU, LeakyRelu（alpha=0.2）替换为PReLU（）。
# Keras 当前没有RReLU的官方实现，但是你可以轻松地实现自己的
# SELU
layer = keras.layers.Dense(10, activation="selu",
		kernel_initializer="lecun_normal")

```



### 批量归一化  

尽管将He初始化与ELU（或ReLU的任何变体）一起使用可以显著减少在训练开始时的梯度消失/梯度爆炸问题的危险，但这并不能保证它们在训练期间不会再出现。  

在2015年的一篇论文中[8]，Sergey Ioffe和Christian Szegedy提出了一种称为批量归一化（BN）的技术来解决这些问题。该技术包括在模型中的每个隐藏层的激活函数之前或之后添加一个操作。该操作对每个输入零中心并归一化，然后每层使用两个新的参数向量缩放和偏移其结果：一个用于缩放，另一个用于偏移。换句话说，该操作可以使模型学习各层输入的最佳缩放和均值  

$\boldsymbol{\mu}_{\mathrm{B}}=\frac{1}{m_{\mathrm{B}}} \sum_{i=1}^{m_{\mathrm{B}}} \boldsymbol{x}^{(i)}$
$\boldsymbol{\sigma}_{\mathrm{B}}^{2}=\frac{1}{m_{\mathrm{B}}} \sum_{i=1}^{m_{\mathrm{B}}}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathrm{B}}\right)^{2}$
$\hat{\boldsymbol{x}}^{(i)}=\frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathrm{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathrm{B}}^{2}+\boldsymbol{\varepsilon}}}$
$\boldsymbol{z}^{(i)}=\boldsymbol{\gamma} \otimes \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}$

```python
# Keras实现批量归一化
# 只需在每个隐藏层的激活函数之前或之后添加一个BatchNormalization层，然后可选地在模型的第一层后添加一个BN层。
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```



缓解梯度爆炸问题的另一种流行技术是在反向传播期间**裁剪梯度**，使它们永远不会超过某个阈值。这称为梯度裁剪[12]。这种技术最常用于**循环神经网络**。



LSTM不太容易发生梯度消失，主要原因在于LSTM内部复杂的“门（gates）



## 重用预训练层 Reusing Pretrained Layers

迁移学习Transfer learning  

最适合使用深度卷积神经网络，该神经网络倾向于学习更为通用的特征检测器  

![image-20211011162027158](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011162027158.png)



***Figure 11-4. Reusing pretrained layers***  



- 通常应该替换掉原始模型的输出层，因为它对于新任务很有可能根本没有用，甚至对于新任务而言，可能没有正确数量的输出。
- 类似地，原始模型的上部分隐藏层不太可能像下部分那样有用，因为对新任务最有用的高级特征可能与对原始任务最有用的特征有很大的不同。你需要找到正确的层数来重用。
- 任务越相似，可重用的层越多（从较低的层开始）。对于非常相似的任务，请尝试保留所有的隐藏层和只是替换掉输出层。  

how to？

首先尝试冻结所有可重复使用的层（使其权重不可训练，这样梯度下降就不会对其进行修改），训练模型并查看其表现。然后尝试解冻上部隐藏层中的一两层，使反向传播可以对其进行调整，再查看性能是否有所提高。  



```python
model_A = keras.models.load_model("my_model_A.h5")
model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))
# 请注意，model_A和model_B_on_A现在共享一些层。当训练model_B_on_A时，也会影响model_A。
model_A_clone = keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())

for layer in model_B_on_A.layers[:-1]:
	layer.trainable = False
model_B_on_A.compile(loss="binary_crossentropy", optimizer="sgd",metrics=["accuracy"])

history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,validation_data=(X_valid_B, y_valid_B))
for layer in model_B_on_A.layers[:-1]:
	layer.trainable = True
optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,validation_data=(X_valid_B, y_valid_B))
```



## 更快的优化器 Faster Optimizers  

上述方法（对连接权重应用一个良好的初始化策略，使用良好的激活函数，使用批量归一化，以及重用预训练网络的某些部分  ）

能够加快训练速度，并获得更好的解

### 动量优化

1. $\mathbf{m} \leftarrow \beta \mathbf{m}-\eta \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta})$
2. $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\mathbf{m}$

梯度下降相当快地沿着陡峭的斜坡下降，但是沿着山谷下降需要很长时间。相反，动量优化将沿着山谷滚动得越来越快，直到达到谷底（最优解）。  

```python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)
```



由于这种动量势头，优化器可能会稍微过调，然后又回来，再次过调，在稳定于最小点之前会多次振荡。这是在系统中有一些摩擦力的原因之一：它消除了这些振荡，从而加快了收敛速度。  

动量优化的一个缺点是它增加了另一个超参数来调整。但是，动量值为0.9通常在实践中效果很好，几乎总是比常规的“梯度下降”更快。  



### Nesterov Accelerated Gradient

1. $\mathbf{m} \leftarrow \beta \mathbf{m}-\eta \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}+\beta \mathbf{m})$
2. $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\mathbf{m}$

NAG通常比常规动量优化更快 

```python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)
```



### AdaGrad  

daGrad算法[3]通过沿最陡峭的维度按比例缩小梯度向量（见公式11-6）来实现此校正  

1. $\mathbf{s} \leftarrow \mathbf{s}+\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) \otimes \nabla_{\boldsymbol{\theta}} \boldsymbol{J}(\boldsymbol{\theta})$
2. $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\eta \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) \oslash \sqrt{\mathbf{s}+\boldsymbol{\varepsilon}}$

$\otimes $ 符号表示逐元素相乘 ；$\oslash$ 符号代表逐元素相除 ；$\varepsilon$ 是避免除以零的平滑项，通常设置为$10^{-10}$

简而言之，该算法会降低学习率，但是对于陡峭的维度，它的执行速度要比对缓慢下降的维度的执行速度要快。 这称为自适应学习率。它有助于将结果更新更直接地指向全局最优解（见图11-7）。另一个好处是，它几乎不需要调整学习率超参数η。 

**这种方法不适合训练DNN，但对线性回归之类的简单任务可能有效**



### Adam & Nadam

Adam[5]代表自适应矩估计，结合了动量优化和RMSProp的思想：就像动量优化一样，它跟踪过去梯度的指数衰减平均值。  

1. $\boldsymbol{m} \leftarrow \beta_{1} \boldsymbol{m}-\left(1-\beta_{1}\right) \nabla_{\theta} J(\boldsymbol{\theta})$
2. $\quad \boldsymbol{s} \leftarrow \beta_{2} \boldsymbol{s}+\left(1-\beta_{2}\right) \nabla_{\theta} J(\boldsymbol{\theta}) \otimes \nabla_{\theta} J(\boldsymbol{\theta})$
3. $\hat{\boldsymbol{m}} \leftarrow \frac{m}{1-\beta_{1}^{t}}$
4. $\hat{s} \leftarrow \frac{s}{1-\beta_{2}^{t}}$
5. $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\eta \hat{\boldsymbol{m}} \oslash \sqrt{\hat{\boldsymbol{s}}+\varepsilon}$



```python
optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
```



> 注意，一般都基于一阶偏导数，二阶输出每个输出都有 $n^2$ 个Hessian。由于DNN通常有成千上万个参数，二阶太慢



**如何选择优化器：**

![image-20211011165713160](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011165713160.png)





###  学习率

学习率过高：训练可能会发散

偏高：次优解

偏低：收敛太慢

![image-20211011165759747](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011165759747.png)

***Figure 11-8. Learning curves for various learning rates η***



**从一个高的学习率开始，然后降低！**

但是，你可以做得比恒定学习率更好：  从一个较大的学习率开始，一旦训练没有取得进展后就降低它，那与恒定学习率相比，你就可以更快地找到一个最优解。  

- Power scheduling

  η(t) = η / (1 + t/s) .  其中 s = steps

- Exponential scheduling

  $η(t) = η 0.1^{t/s}$

- Piecewise constant scheduling

- 1cycle scheduling

```python
# power
optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)

# exponential
def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
    	return lr0 * 0.1**(epoch / s)
	return exponential_decay_fn
exponential_decay_fn = exponential_decay(lr0=0.01, s=20)

lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])
```



## 通过正则化避免过拟合  

深度神经网络通常具有数万个参数，有时甚至有数百万个。这给它们带来了难以置信的自由度，意味着它们可以拟合各种各样的复杂数据集。但是，这种巨大的灵活性也使网络易于过拟合训练集。我们需要正则化。我们已经在第10章中实现了最好的正则化技术之一：提前停止 。

### 11.4.1　l1和l2正则化  

就像在第4章中对简单线性模型所做的一样，可以使用l2正则化来约束神经网络的连接权重，如果想要稀疏模型（许多权重等于0）则可以使用l1正则化。  

```python
# l2正则化
layer = keras.layers.Dense(100, activation="elu",
        kernel_initializer="he_normal",
        kernel_regularizer=keras.regularizers.l2(0.01))
# l1正则化：keras.regularizers.l1()
# 同时需要l1和l2正则化，请使用keras.regularizers.l1_l2()
```





### drop out

在每个训练步骤中，每个神经元（包括输入神经元，但始终不包括输出神经元）都有暂时“删除”的概率 $p$ 。这意味着在这个训练步骤中它被完全忽略，但在下一步中可能处于活动状态。

 

```python
# 要使用Keras实现dropout，可以使用keras.layers.Dropout层。
model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.Dropout(rate=0.2),
        keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
        keras.layers.Dropout(rate=0.2),
        keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    	keras.layers.Dropout(rate=0.2),
		keras.layers.Dense(10, activation="softmax")
])
```

如果你发现模型过拟合，则可以提高dropout率。相反，如果模型欠拟合训练集，则应尝试降低dropout率。  

如果要基于**SELU激活函数**（如前所述）对自归一化网络进行正则化，则应使用**alpha dropout**：这是dropout的一种变体，它保留了其输入的均值和标准差（在与SELU相同的论文中介绍，因为常规的dropout会破坏自归一化）。  

### 蒙特卡罗（MC）Dropout  



### 最大范数正则化  

默认的DNN配置

![image-20211011184516637](https://hl-pic.oss-cn-hangzhou.aliyuncs.com/image-20211011184516637.png)

如果网络是密集层的简单堆叠，则它可以自归一化。使用自归一化的DNN配置

| Hyperparameter         | Default value                               |
| ---------------------- | ------------------------------------------- |
| Kernel initializer     | LeCun initialization                        |
| Activation function    | SELU                                        |
| Normalization          | None (self-normalization)                   |
| Regularization         | Alpha dropout if needed                     |
| Optimizer              | Momentum optimization (or RMSProp or Nadam) |
| Learning rate schedule | 1cycle                                      |

需要注意的是：

- 不要忘了归一化输入特征！
- 如果可以找到解决类似问题的神经网络，那应该尝试重用部分神经网络；
- 如果有大量未标记的数据，则应使用无监督预训练；如果有相似任务的大量标记的数据，则应该在辅助任务上使用预训练。

以下是一些例外情况：

- 如果需要稀疏模型，则可以使用1正则化（可以选择在训练后将很小的权重归零）。如果你需要更稀疏的模型，则可以使用TensorFlow模型优化工具包。这会破坏自归一化，因此在这种情况下，你应使用默认配置
- 如果你需要低延迟的模型（执行闪电般快速预测的模型），则可能需要使用更少的层，将批量归一化层融合到先前的层中，并使用更快的激活函数，例如leaky ReLU或仅仅使用ReLU。拥有稀疏模型也将有所帮助。最后，你可能想把浮点精度从32位降低到16位甚至8位（见19.2节）。再一次检查TFMOT。
- 如果你要构建风险敏感的应用，或者推理延迟在你的应用中不是很重要，则可以使用MC Dropout来提高性能并获得更可靠的概率估计以及不确定性估计。  

